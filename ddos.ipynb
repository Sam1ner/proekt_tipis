{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "%delete_livy_session --cluster bigdata-course-spark-cluster --id gr1391_Kainelainen_Teterin",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "%create_livy_session \\\n--cluster bigdata-course-spark-cluster \\\n--id gr1391_Kainelainen_Teterin \\\n--conf spark.executor.instances=1 \\\n--conf spark.executor.cores=1",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nfilePathUnbalanced = \"/user/bigdata-course/DDoS Dataset/ddos_imbalanced/unbalaced_20_80_dataset.csv\"\ndf = spark.read.csv(filePathUnbalanced,\n                    header=True,\n                    inferSchema=True)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.sql.functions import desc\n\ndfSelected = df.limit(2_250_000)\ndfSelected = dfSelected.withColumn(\"_c0\", monotonically_increasing_id()).orderBy(desc(\"_c0\")).limit(2_000_000)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nstringCols = []\noutputStringCols = []\nfor i in range(len(df.columns)):\n    if df.dtypes[i][1] == \"string\":\n        stringCols.append(df.dtypes[i][0])\n        outputStringCols.append('indexed_' + df.dtypes[i][0])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlabelIndexer = StringIndexer(inputCols=stringCols, outputCols=outputStringCols, handleInvalid=\"skip\").fit(dfSelected)\ndfSelected = labelIndexer.transform(dfSelected)\ndfSelected = dfSelected.drop(*stringCols)\n\nfeatures = dfSelected.columns.copy()\nfeatures.remove(\"_c0\")\nfeatures.remove(\"indexed_Label\")\nfeatures.remove(\"indexed_Flow ID\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nva = VectorAssembler(\n    inputCols=features,\n    outputCol=\"Features\",\n    handleInvalid=\"skip\"\n).transform(dfSelected)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nimport time\nstart_time = time.time()\nfrom pyspark.ml.classification import GBTClassifier\n\ntrain, test = va.randomSplit([0.7, 0.3])\nclassifier = GBTClassifier(\n    labelCol=\"indexed_Label\",\n    featuresCol=\"Features\",\n    maxIter=3,\n    maxBins=dfSelected.count()\n)\nmodel = classifier.fit(train)\n\nprediction = model.transform(test)\nprediction.show(n=1, vertical=True)\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"f1\")\naccuracy = evaluator.evaluate(prediction)\nprint(accuracy)\nprint(f\"Test Error = {1.0 - accuracy}\")\n\nprint(f\"Execution time: {time.time() - start_time}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!spark --session gr1391_Kainelainen_Teterin\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"weightedPrecision\") \naccuracy = evaluator.evaluate(prediction)\nprint(f\"Weighted Precision: {accuracy}\")\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"logLoss\") \naccuracy = evaluator.evaluate(prediction)\nprint(f\"Log Loss: {accuracy}\")\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"weightedRecall\") \naccuracy = evaluator.evaluate(prediction)\nprint(f\"Weighted Recall: {accuracy}\")\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"weightedTruePositiveRate\") \naccuracy = evaluator.evaluate(prediction)\nprint(f\"Weighted True Positive Rate: {accuracy}\")\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Label\", \n                                              predictionCol=\"prediction\", \n                                              metricName=\"weightedFMeasure\") \naccuracy = evaluator.evaluate(prediction)\nprint(f\"Weighted F-Measure: {accuracy}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}